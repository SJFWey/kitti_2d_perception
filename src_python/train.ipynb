{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "caea4557",
      "metadata": {},
      "source": [
        "# KITTI 2D Object Detection Training\n",
        "\n",
        "This notebook expects you to download the KITTI 2D detection dataset yourself and set `DATASET_ROOT` to the dataset root directory.\n",
        "\n",
        "Expected structure:\n",
        "- data_object_image_2/training/image_2\n",
        "- data_object_label_2/training/label_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c67882",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "subprocess.check_call(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"opencv-python\", \"matplotlib\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74c5de6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.ops import box_iou\n",
        "from torch.optim.sgd import SGD\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "515e52aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    IMG_HEIGHT: int = 384\n",
        "    IMG_WIDTH: int = 1248\n",
        "\n",
        "    TRAIN_BATCH_SIZE: int = 8\n",
        "    VAL_BATCH_SIZE: int = 8\n",
        "\n",
        "    BASE_LR: float = 0.01\n",
        "    BACKBONE_LR_MULT: float = 0.1\n",
        "\n",
        "    MAX_EPOCHS: int = 24\n",
        "    SEED: int = 42\n",
        "    VAL_SIZE: int = 1000\n",
        "\n",
        "    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    SCORE_THRESH_EVAL: float = 0.05\n",
        "    MAX_DETECTIONS_PER_IMAGE: int = 100\n",
        "    IOU_THRESHOLDS: Tuple[float, ...] = tuple(np.round(np.arange(0.50, 0.96, 0.05), 2).tolist())\n",
        "\n",
        "    LR_PLATEAU_FACTOR: float = 0.5\n",
        "    LR_PLATEAU_PATIENCE: int = 2\n",
        "    LR_MIN: float = 1e-5\n",
        "\n",
        "    GRAD_CLIP_NORM: float = 2.0\n",
        "\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "SAVE_DIR = Path(\"outputs/train_v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f75e00d",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_ROOT = Path(\"{YOUR_PROJECT}/data/kitti_detection/\")  # TODO: change to your KITTI root\n",
        "IMAGES_DIR = DATASET_ROOT / \"data_object_image_2\" / \"training\" / \"image_2\"\n",
        "LABELS_DIR = DATASET_ROOT / \"data_object_label_2\" / \"training\" / \"label_2\"\n",
        "\n",
        "if not IMAGES_DIR.exists() or not LABELS_DIR.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"KITTI paths not found. Set DATASET_ROOT so that \"\n",
        "        \"data_object_image_2/training/image_2 and \"\n",
        "        \"data_object_label_2/training/label_2 exist.\"\n",
        "    )\n",
        "\n",
        "print(f\"--> Images: {IMAGES_DIR}\")\n",
        "print(f\"--> Labels: {LABELS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0a18f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42) -> None:\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def ensure_dir(path: Path) -> None:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def config_to_dict() -> dict:\n",
        "    cfg = asdict(Config())\n",
        "    cfg[\"DEVICE\"] = str(cfg[\"DEVICE\"])\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def plot_history(history: List[dict], save_dir: Path) -> None:\n",
        "    if not history:\n",
        "        print(\"--> No history to plot.\")\n",
        "        return\n",
        "\n",
        "    import matplotlib\n",
        "\n",
        "    matplotlib.use(\"Agg\")\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    epochs = [h.get(\"epoch\") for h in history]\n",
        "    train_loss = [h.get(\"train_loss\") for h in history]\n",
        "    val_loss = [h.get(\"val_loss\") for h in history]\n",
        "    map50 = [h.get(\"map50\") for h in history]\n",
        "    map5095 = [h.get(\"map5095\") for h in history]\n",
        "\n",
        "    lrs = [h.get(\"lrs\", []) for h in history]\n",
        "    lr_groups: List[List[float]] = []\n",
        "    if lrs and lrs[0]:\n",
        "        num_groups = len(lrs[0])\n",
        "        for gi in range(num_groups):\n",
        "            lr_groups.append(\n",
        "                [float(v[gi]) if v and gi < len(v) else float(\"nan\") for v in lrs]\n",
        "            )\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    axes[0].plot(epochs, train_loss, label=\"train_loss\")\n",
        "    axes[0].plot(epochs, val_loss, label=\"val_loss\")\n",
        "    axes[0].set_xlabel(\"epoch\")\n",
        "    axes[0].set_ylabel(\"loss\")\n",
        "    axes[0].set_title(\"Loss\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot(epochs, map50, label=\"mAP@0.50\")\n",
        "    axes[1].plot(epochs, map5095, label=\"mAP@0.50:0.95\")\n",
        "    axes[1].set_xlabel(\"epoch\")\n",
        "    axes[1].set_ylabel(\"mAP\")\n",
        "    axes[1].set_title(\"Metrics\")\n",
        "    axes[1].set_ylim(0.0, 1.0)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].legend()\n",
        "\n",
        "    if lr_groups:\n",
        "        for gi, vals in enumerate(lr_groups):\n",
        "            axes[2].plot(epochs, vals, label=f\"lr_g{gi}\")\n",
        "        lr_values = [v for group in lr_groups for v in group if np.isfinite(v)]\n",
        "        if lr_values and all(v > 0 for v in lr_values):\n",
        "            axes[2].set_yscale(\"log\")\n",
        "        axes[2].set_xlabel(\"epoch\")\n",
        "        axes[2].set_ylabel(\"lr\")\n",
        "        axes[2].set_title(\"Learning Rate\")\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        axes[2].legend()\n",
        "    else:\n",
        "        axes[2].text(0.5, 0.5, \"no lr data\", ha=\"center\", va=\"center\")\n",
        "        axes[2].set_axis_off()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    ensure_dir(save_dir)\n",
        "    out_path = save_dir / \"training_curves.png\"\n",
        "    fig.savefig(out_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"--> Curves saved: {out_path}\")\n",
        "\n",
        "\n",
        "def save_training_history(history: List[dict], save_dir: Path, best_path: Path) -> None:\n",
        "    hist_path = save_dir / \"history.npy\"\n",
        "    np.save(hist_path, np.array(history, dtype=object), allow_pickle=True)\n",
        "    print(f\"--> History saved: {hist_path}\")\n",
        "    plot_history(history, save_dir)\n",
        "    print(f\"--> Best model: {best_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "722f09ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "class KittiDataset(Dataset):\n",
        "    def __init__(self, images_dir: Path, labels_dir: Path):\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.labels_dir = Path(labels_dir)\n",
        "\n",
        "        self.class_map = {\"Car\": 1, \"Pedestrian\": 2, \"Cyclist\": 3}\n",
        "        self.id_to_name = {v: k for k, v in self.class_map.items()}\n",
        "\n",
        "        if not self.images_dir.exists():\n",
        "            raise FileNotFoundError(f\"Images directory not found: {self.images_dir}\")\n",
        "        if not self.labels_dir.exists():\n",
        "            raise FileNotFoundError(f\"Labels directory not found: {self.labels_dir}\")\n",
        "\n",
        "        self.imgs = sorted(\n",
        "            [\n",
        "                f\n",
        "                for f in os.listdir(self.images_dir)\n",
        "                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, sample_idx: int):\n",
        "        img_name = self.imgs[sample_idx]\n",
        "        img_path = self.images_dir / img_name\n",
        "\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f\"Failed to read image: {img_path}\")\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        orig_h, orig_w, _ = img.shape\n",
        "\n",
        "        img = cv2.resize(img, (Config.IMG_WIDTH, Config.IMG_HEIGHT), interpolation=cv2.INTER_LINEAR)\n",
        "        img /= 255.0\n",
        "\n",
        "        label_file = self.labels_dir / (Path(img_name).stem + \".txt\")\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        if label_file.exists():\n",
        "            with open(label_file, \"r\") as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if not parts:\n",
        "                        continue\n",
        "                    cls = parts[0]\n",
        "                    if len(parts) < 8:\n",
        "                        continue\n",
        "                    if cls in self.class_map:\n",
        "                        label_id = self.class_map[cls]\n",
        "                        xmin, ymin, xmax, ymax = map(float, parts[4:8])\n",
        "                        boxes.append([xmin, ymin, xmax, ymax])\n",
        "                        labels.append(label_id)\n",
        "\n",
        "        if boxes:\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "            w_scale = Config.IMG_WIDTH / float(orig_w)\n",
        "            h_scale = Config.IMG_HEIGHT / float(orig_h)\n",
        "\n",
        "            boxes[:, [0, 2]] *= w_scale\n",
        "            boxes[:, [1, 3]] *= h_scale\n",
        "\n",
        "            boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(0, Config.IMG_WIDTH - 1)\n",
        "            boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(0, Config.IMG_HEIGHT - 1)\n",
        "\n",
        "            keep = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
        "            boxes = boxes[keep]\n",
        "            labels = labels[keep]\n",
        "        else:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor(sample_idx, dtype=torch.int64),\n",
        "        }\n",
        "\n",
        "        img_tensor = torch.from_numpy(img).permute(2, 0, 1).contiguous()\n",
        "        return img_tensor, target\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "def move_batch_to_device(images, targets, device):\n",
        "    images = [img.to(device) for img in images]\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "    return images, targets\n",
        "\n",
        "\n",
        "def build_dataloader(dataset, batch_size: int, shuffle: bool) -> DataLoader:\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "310e6f84",
      "metadata": {},
      "outputs": [],
      "source": [
        "def freeze_batchnorm(module: torch.nn.Module) -> None:\n",
        "    for m in module.modules():\n",
        "        if isinstance(m, torch.nn.BatchNorm2d):\n",
        "            m.eval()\n",
        "\n",
        "\n",
        "def build_model(num_classes: int) -> torch.nn.Module:\n",
        "    from torchvision.models.detection import (\n",
        "        fasterrcnn_resnet50_fpn_v2,\n",
        "        FasterRCNN_ResNet50_FPN_V2_Weights,\n",
        "    )\n",
        "\n",
        "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "    model = fasterrcnn_resnet50_fpn_v2(\n",
        "        weights=weights,\n",
        "        min_size=Config.IMG_HEIGHT,\n",
        "        max_size=Config.IMG_WIDTH,\n",
        "    )\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
        "        in_features, num_classes\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_optimizer(model: torch.nn.Module) -> torch.optim.Optimizer:\n",
        "    backbone_params = []\n",
        "    head_params = []\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if name.startswith(\"backbone.\"):\n",
        "            backbone_params.append(p)\n",
        "        else:\n",
        "            head_params.append(p)\n",
        "\n",
        "    param_groups = [\n",
        "        {\"params\": backbone_params, \"lr\": Config.BASE_LR * Config.BACKBONE_LR_MULT},\n",
        "        {\"params\": head_params, \"lr\": Config.BASE_LR},\n",
        "    ]\n",
        "\n",
        "    optimizer = SGD(param_groups, lr=Config.BASE_LR, momentum=0.9, weight_decay=1e-4)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def build_lr_scheduler(optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.ReduceLROnPlateau:\n",
        "    return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"max\",\n",
        "        factor=Config.LR_PLATEAU_FACTOR,\n",
        "        patience=Config.LR_PLATEAU_PATIENCE,\n",
        "        min_lr=Config.LR_MIN,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3b48d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_torch_save(payload: dict, path: Path) -> None:\n",
        "    ensure_dir(path.parent)\n",
        "    tmp_path = path.with_suffix(path.suffix + \".tmp\")\n",
        "    torch.save(payload, tmp_path)\n",
        "    os.replace(tmp_path, path)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model, data_loader, device):\n",
        "    was_training = model.training\n",
        "    model.train()\n",
        "    freeze_batchnorm(model)\n",
        "\n",
        "    val_loss_sum = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images, targets = move_batch_to_device(images, targets, device)\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(v for v in loss_dict.values())\n",
        "\n",
        "        val_loss_sum += float(loss.detach().item())\n",
        "        num_batches += 1\n",
        "\n",
        "    if not was_training:\n",
        "        model.eval()\n",
        "\n",
        "    return val_loss_sum / max(1, num_batches)\n",
        "\n",
        "\n",
        "def _ap_from_pr_101(recall: np.ndarray, precision: np.ndarray) -> float:\n",
        "    if recall.size == 0:\n",
        "        return 0.0\n",
        "\n",
        "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
        "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
        "\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n",
        "\n",
        "    recall_levels = np.linspace(0.0, 1.0, 101)\n",
        "    prec_at_recall = np.zeros_like(recall_levels)\n",
        "\n",
        "    for i, r in enumerate(recall_levels):\n",
        "        inds = np.where(mrec >= r)[0]\n",
        "        prec_at_recall[i] = np.max(mpre[inds]) if inds.size > 0 else 0.0\n",
        "\n",
        "    return float(np.mean(prec_at_recall))\n",
        "\n",
        "\n",
        "def _compute_ap_for_class(\n",
        "    detections: List[Tuple[int, torch.Tensor, float]],\n",
        "    gts: Dict[int, torch.Tensor],\n",
        "    iou_thr: float,\n",
        ") -> Optional[float]:\n",
        "    npos = int(sum(int(v.shape[0]) for v in gts.values()))\n",
        "    if npos == 0:\n",
        "        return None\n",
        "\n",
        "    dets = sorted(detections, key=lambda x: x[2], reverse=True)\n",
        "    if len(dets) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    matched: Dict[int, torch.Tensor] = {}\n",
        "    for img_id, gt_boxes in gts.items():\n",
        "        matched[img_id] = torch.zeros((gt_boxes.shape[0],), dtype=torch.bool)\n",
        "\n",
        "    tp = np.zeros((len(dets),), dtype=np.float32)\n",
        "    fp = np.zeros((len(dets),), dtype=np.float32)\n",
        "\n",
        "    empty = torch.zeros((0, 4), dtype=torch.float32)\n",
        "\n",
        "    for i, (img_id, det_box, det_score) in enumerate(dets):\n",
        "        gt_boxes = gts.get(img_id, empty)\n",
        "        if gt_boxes.numel() == 0:\n",
        "            fp[i] = 1.0\n",
        "            continue\n",
        "\n",
        "        ious = box_iou(det_box.unsqueeze(0), gt_boxes).squeeze(0)\n",
        "        max_iou, max_j = torch.max(ious, dim=0)\n",
        "        max_iou_v = float(max_iou.item())\n",
        "        j = int(max_j.item())\n",
        "\n",
        "        if max_iou_v >= iou_thr and (not bool(matched[img_id][j].item())):\n",
        "            tp[i] = 1.0\n",
        "            matched[img_id][j] = True\n",
        "        else:\n",
        "            fp[i] = 1.0\n",
        "\n",
        "    tp_cum = np.cumsum(tp)\n",
        "    fp_cum = np.cumsum(fp)\n",
        "\n",
        "    recall = tp_cum / (npos + 1e-12)\n",
        "    precision = tp_cum / (tp_cum + fp_cum + 1e-12)\n",
        "\n",
        "    return _ap_from_pr_101(recall, precision)\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate_map(\n",
        "    model: torch.nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    class_ids: List[int],\n",
        "    iou_thresholds: Tuple[float, ...],\n",
        "    score_thresh: float,\n",
        "    max_dets_per_image: int,\n",
        ") -> Dict[str, float]:\n",
        "    model.eval()\n",
        "\n",
        "    gts_by_class: Dict[int, Dict[int, torch.Tensor]] = {c: {} for c in class_ids}\n",
        "    dets_by_class: Dict[int, List[Tuple[int, torch.Tensor, float]]] = {c: [] for c in class_ids}\n",
        "\n",
        "    for images, targets in tqdm(data_loader, desc=\"Eval mAP\", leave=False):\n",
        "        images = [img.to(device) for img in images]\n",
        "        outputs = model(images)\n",
        "\n",
        "        for out, tgt in zip(outputs, targets):\n",
        "            img_id = int(tgt[\"image_id\"].item())\n",
        "\n",
        "            gt_boxes = tgt[\"boxes\"].cpu()\n",
        "            gt_labels = tgt[\"labels\"].cpu()\n",
        "\n",
        "            for c in class_ids:\n",
        "                mask = gt_labels == c\n",
        "                gts_by_class[c][img_id] = gt_boxes[mask].float()\n",
        "\n",
        "            pred_boxes = out[\"boxes\"].detach().cpu().float()\n",
        "            pred_scores = out[\"scores\"].detach().cpu().float()\n",
        "            pred_labels = out[\"labels\"].detach().cpu().long()\n",
        "\n",
        "            keep = pred_scores >= float(score_thresh)\n",
        "            pred_boxes = pred_boxes[keep]\n",
        "            pred_scores = pred_scores[keep]\n",
        "            pred_labels = pred_labels[keep]\n",
        "\n",
        "            if pred_scores.numel() > max_dets_per_image:\n",
        "                topk = torch.topk(pred_scores, k=max_dets_per_image, largest=True).indices\n",
        "                pred_boxes = pred_boxes[topk]\n",
        "                pred_scores = pred_scores[topk]\n",
        "                pred_labels = pred_labels[topk]\n",
        "\n",
        "            for c in class_ids:\n",
        "                cmask = pred_labels == c\n",
        "                if cmask.any():\n",
        "                    boxes_c = pred_boxes[cmask]\n",
        "                    scores_c = pred_scores[cmask]\n",
        "                    for b, s in zip(boxes_c, scores_c):\n",
        "                        dets_by_class[c].append((img_id, b, float(s.item())))\n",
        "\n",
        "    ap_per_thr: Dict[float, List[float]] = {t: [] for t in iou_thresholds}\n",
        "    ap50_per_class: Dict[int, float] = {}\n",
        "\n",
        "    for c in class_ids:\n",
        "        gt_c = gts_by_class[c]\n",
        "        det_c = dets_by_class[c]\n",
        "\n",
        "        ap_list_for_c = []\n",
        "        for t in iou_thresholds:\n",
        "            ap = _compute_ap_for_class(det_c, gt_c, float(t))\n",
        "            ap_list_for_c.append(ap)\n",
        "\n",
        "        if ap_list_for_c[0] is not None:\n",
        "            ap50_per_class[c] = float(ap_list_for_c[0])\n",
        "\n",
        "        for t, ap in zip(iou_thresholds, ap_list_for_c):\n",
        "            if ap is not None:\n",
        "                ap_per_thr[t].append(float(ap))\n",
        "\n",
        "    map50 = float(np.mean(ap_per_thr.get(0.50, []))) if ap_per_thr.get(0.50, []) else 0.0\n",
        "    map5095_vals = []\n",
        "    for t in iou_thresholds:\n",
        "        vals = ap_per_thr.get(t, [])\n",
        "        if vals:\n",
        "            map5095_vals.append(float(np.mean(vals)))\n",
        "    map5095 = float(np.mean(map5095_vals)) if map5095_vals else 0.0\n",
        "\n",
        "    out_metrics: Dict[str, float] = {\n",
        "        \"mAP@0.50\": map50,\n",
        "        \"mAP@0.50:0.95\": map5095,\n",
        "    }\n",
        "    for c, ap in ap50_per_class.items():\n",
        "        out_metrics[f\"AP@0.50_cls{c}\"] = ap\n",
        "\n",
        "    return out_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a7fe6ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(images_dir: Path, labels_dir: Path) -> None:\n",
        "    seed_everything(Config.SEED)\n",
        "\n",
        "    print(f\"--> torch={torch.__version__} torchvision={torchvision.__version__}\")\n",
        "    print(f\"--> Device: {Config.DEVICE}\")\n",
        "\n",
        "    save_dir = SAVE_DIR\n",
        "    ensure_dir(save_dir)\n",
        "    print(f\"--> Save dir: {save_dir}\")\n",
        "\n",
        "    dataset = KittiDataset(images_dir, labels_dir)\n",
        "    if len(dataset) < 2:\n",
        "        raise RuntimeError(\"Dataset too small.\")\n",
        "\n",
        "    val_size = min(Config.VAL_SIZE, max(1, len(dataset) // 10))\n",
        "    train_size = len(dataset) - val_size\n",
        "\n",
        "    g = torch.Generator().manual_seed(Config.SEED)\n",
        "    dataset_train, dataset_val = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size], generator=g\n",
        "    )\n",
        "\n",
        "    train_loader = build_dataloader(dataset_train, Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
        "    val_loader = build_dataloader(dataset_val, Config.VAL_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(f\"--> Training samples: {len(dataset_train)} | Val samples: {len(dataset_val)}\")\n",
        "\n",
        "    model = build_model(num_classes=NUM_CLASSES)\n",
        "    model.to(Config.DEVICE)\n",
        "\n",
        "    optimizer = build_optimizer(model)\n",
        "    lr_scheduler = build_lr_scheduler(optimizer)\n",
        "\n",
        "    class_ids = sorted(list(dataset.class_map.values()))\n",
        "    id_to_name = dataset.id_to_name\n",
        "\n",
        "    best_map50 = -1.0\n",
        "    best_path = save_dir / \"best_model.pth\"\n",
        "    last_path = save_dir / \"last.pth\"\n",
        "\n",
        "    history: List[dict] = []\n",
        "\n",
        "    print(\"--> Sanity check: one forward pass...\")\n",
        "    model.train()\n",
        "    freeze_batchnorm(model)\n",
        "\n",
        "    images, targets = next(iter(train_loader))\n",
        "    images, targets = move_batch_to_device(images, targets, Config.DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(v for v in loss_dict.values())\n",
        "    if not torch.isfinite(loss):\n",
        "        raise RuntimeError(f\"Sanity check loss is not finite: {loss.item()}\")\n",
        "    print({k: float(v) for k, v in loss_dict.items()})\n",
        "    print(f\"--> Sanity check total loss: {float(loss)}\")\n",
        "\n",
        "    print(\"--> Starting training...\")\n",
        "    last_checkpoint_state = None\n",
        "    interrupted = False\n",
        "\n",
        "    try:\n",
        "        for epoch in range(Config.MAX_EPOCHS):\n",
        "            model.train()\n",
        "            freeze_batchnorm(model)\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            t0 = time.time()\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{Config.MAX_EPOCHS}\", leave=True)\n",
        "            for images, targets in pbar:\n",
        "                images, targets = move_batch_to_device(images, targets, Config.DEVICE)\n",
        "\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                loss_dict = model(images, targets)\n",
        "                losses = sum(v for v in loss_dict.values())\n",
        "\n",
        "                losses.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=Config.GRAD_CLIP_NORM)\n",
        "                optimizer.step()\n",
        "\n",
        "                loss_val = float(losses.detach().item())\n",
        "                epoch_loss += loss_val\n",
        "                pbar.set_postfix(loss=f\"{loss_val:.4f}\")\n",
        "\n",
        "            train_loss = epoch_loss / max(1, len(train_loader))\n",
        "            val_loss = evaluate_loss(model, val_loader, Config.DEVICE)\n",
        "\n",
        "            val_metrics = evaluate_map(\n",
        "                model=model,\n",
        "                data_loader=val_loader,\n",
        "                device=Config.DEVICE,\n",
        "                class_ids=class_ids,\n",
        "                iou_thresholds=Config.IOU_THRESHOLDS,\n",
        "                score_thresh=Config.SCORE_THRESH_EVAL,\n",
        "                max_dets_per_image=Config.MAX_DETECTIONS_PER_IMAGE,\n",
        "            )\n",
        "\n",
        "            map50 = float(val_metrics.get(\"mAP@0.50\", 0.0))\n",
        "            map5095 = float(val_metrics.get(\"mAP@0.50:0.95\", 0.0))\n",
        "\n",
        "            lr_scheduler.step(map50)\n",
        "\n",
        "            current_lrs = [pg[\"lr\"] for pg in optimizer.param_groups]\n",
        "            dt = time.time() - t0\n",
        "\n",
        "            per_class_str_parts = []\n",
        "            for c in class_ids:\n",
        "                k = f\"AP@0.50_cls{c}\"\n",
        "                if k in val_metrics:\n",
        "                    per_class_str_parts.append(f\"{id_to_name.get(c, str(c))}:{val_metrics[k]:.3f}\")\n",
        "            per_class_str = \" \".join(per_class_str_parts) if per_class_str_parts else \"n/a\"\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch + 1} | time {dt:.1f}s | \"\n",
        "                f\"train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | \"\n",
        "                f\"mAP@0.50 {map50:.4f} | mAP@0.50:0.95 {map5095:.4f} | \"\n",
        "                f\"AP@0.50 per class {per_class_str} | \"\n",
        "                f\"lr {current_lrs}\"\n",
        "            )\n",
        "\n",
        "            history.append(\n",
        "                {\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"train_loss\": train_loss,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"map50\": map50,\n",
        "                    \"map5095\": map5095,\n",
        "                    \"lrs\": current_lrs,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            is_best = map50 > best_map50\n",
        "            if is_best:\n",
        "                best_map50 = map50\n",
        "\n",
        "            checkpoint_state = {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"map50\": map50,\n",
        "                \"map5095\": map5095,\n",
        "                \"best_map50\": best_map50,\n",
        "                \"config\": config_to_dict(),\n",
        "                \"history\": history,\n",
        "            }\n",
        "\n",
        "            last_checkpoint_state = checkpoint_state\n",
        "            safe_torch_save(checkpoint_state, last_path)\n",
        "\n",
        "            if is_best:\n",
        "                safe_torch_save(checkpoint_state, best_path)\n",
        "                print(f\"--> Best checkpoint updated: {best_path} (mAP@0.50={best_map50:.4f})\")\n",
        "    except KeyboardInterrupt:\n",
        "        interrupted = True\n",
        "        print(\"--> Training interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(f\"--> Training failed: {e}. Saving last checkpoint...\")\n",
        "        if last_checkpoint_state is not None:\n",
        "            safe_torch_save(last_checkpoint_state, last_path)\n",
        "        save_training_history(history, save_dir, best_path)\n",
        "        raise\n",
        "\n",
        "    if interrupted:\n",
        "        if last_checkpoint_state is not None:\n",
        "            safe_torch_save(last_checkpoint_state, last_path)\n",
        "        save_training_history(history, save_dir, best_path)\n",
        "        return\n",
        "\n",
        "    save_training_history(history, save_dir, best_path)\n",
        "    print(\"--> Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da09b115",
      "metadata": {},
      "outputs": [],
      "source": [
        "main(IMAGES_DIR, LABELS_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
